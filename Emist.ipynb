{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is gona be small pytoch project\n",
    "The goal is two thing lear clasification in pytoch\n",
    "generate images in pytorch.\n",
    "\n",
    "For both of these case i a gona use the EMIST dataset.\n",
    "wich is a opendataset with hanwritten letter convert to images.\n",
    "\n",
    "I like this for two reason one. the problem is simple, second the data size is large but the images size is not wich make it esay to work with. \n",
    "\n",
    "Secondly the problem is also simple.\n",
    "\n",
    "I am thinking about making a small app \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[i am following this](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL Image to PyTorch tensor\n",
    "      transforms.Lambda(lambda x: (x > 0).float())\n",
    "])\n",
    "# Download the EMNIST dataset with the 'letters' split (handwritten letters)\n",
    "train_dataset = datasets.EMNIST(root='./data', split='byclass', train=True, download=True,transform=transform)\n",
    "test_dataset = datasets.EMNIST(root='./data', split='byclass', train=False, download=True,transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "4\n",
      "asigned letter\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#labels\n",
    "#train_dataset.classes\n",
    "#data\n",
    "#train_dataset.target\n",
    "\n",
    "# it seam like each row is array so each train_dataset.data[0] call the first picture train_dataset.data[0][0] is the first row\n",
    "index=100\n",
    "image,label =train_dataset[index]\n",
    "letter_ind=label\n",
    "image = np.transpose(image)  # Swap rows and columns\n",
    "\n",
    "\n",
    "fig = plt.figure\n",
    "plt.imshow(image, cmap='gray_r')\n",
    "plt.show()\n",
    "print(\"label\")\n",
    "print (train_dataset.classes[letter_ind])\n",
    "print(\"asigned letter\")\n",
    "letter_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "train_dataset[0][0].unique()\n",
    "#torch.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasification\n",
    "In this section i am going clasification.\n",
    "for images Convolutional Neural Networks can be used.\n",
    "\n",
    "\n",
    "So the input is gona be one for each input variebl so 28*28=784\n",
    "output is gona be the numer label wich i 10 + the number letters wich 62\n",
    "\n",
    "One of the frustationg thing about neual network is how to chose the number of hiden lays and the number neurons.\n",
    "Their is discussion here.\n",
    "\n",
    "[discussion](https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw)\n",
    "\n",
    "they sai les lair and the average of neuron. but i can alo find discusion that say more layers is better than deep layser.\n",
    "To a exten this makes sens since each layer give a lot combination/interaction, wich is what some this come from but on the other hand i can lead to overfitting.\n",
    "\n",
    "So the ansers in porberli somewhere in between.\n",
    "Which is \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# make nueal network.\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter in the \n",
    "input_size=28*28\n",
    "output_size=len(train_dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self): # start class\n",
    "        super().__init__() # call parent init, so nn.Module\n",
    "        self.flatten = nn.Flatten() # make the input in to a list\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, output_size)  # Output layer with 62 classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.5, inplace=False)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.5, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (13): ReLU()\n",
      "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): Linear(in_features=128, out_features=62, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # This should check if cuda is avaibel ore use cpu\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()  # Loss function for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # define the los functuin\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) #define the optimiser in this case its"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make traning function\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) # the data loader, wich define the bath sizes\n",
    "    model.train() # set model to traning mode \n",
    "    for batch, (X, y) in enumerate(dataloader): # each bach consist of x and y \n",
    "        X, y = X.to(device), y.to(device) \n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad() # this is bit weird it i need to set gradient to zero otherwise it addiativ, This could be if i wanted to use momentum.\n",
    "\n",
    "        if batch % 100 == 0: #print loss every 100 bathes\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) \n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 4.202431  [   64/697932]\n",
      "loss: 4.057902  [ 6464/697932]\n",
      "loss: 4.103806  [12864/697932]\n",
      "loss: 4.065298  [19264/697932]\n",
      "loss: 3.972025  [25664/697932]\n",
      "loss: 3.730175  [32064/697932]\n",
      "loss: 3.744399  [38464/697932]\n",
      "loss: 3.593198  [44864/697932]\n",
      "loss: 3.631562  [51264/697932]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i want to make the app.\n",
    "\n",
    "The idear is to make app that lets me draw a picture from the touch pad.\n",
    "and tries to regenize the the letter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model.pth\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(\"Model loaded successfully.\")\n",
    "else:\n",
    "    print(f\"File {model_path} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
